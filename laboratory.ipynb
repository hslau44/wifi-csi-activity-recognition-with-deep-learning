{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S 0.0 Set parameter and import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_addons as tfa\n",
    "import time\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder,StandardScaler, MinMaxScaler,Normalizer\n",
    "from tensorflow.keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters \n",
    "INPUT_SHAPE = (1000,90,1) \n",
    "ACTIVATION = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function define\n",
    "lrelu = tf.keras.layers.LeakyReLU\n",
    "softmax = tf.keras.layers.Softmax\n",
    "normalization = tfa.layers.InstanceNormalization\n",
    "identity = tf.keras.layers.Lambda(lambda x:x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient reverse layer (Yanin 2015)\n",
    "@tf.custom_gradient\n",
    "def grad_reverse(x):\n",
    "    y = tf.identity(x)\n",
    "    def custom_grad(dy):\n",
    "        return -dy\n",
    "    return y, custom_grad\n",
    "\n",
    "class GradReverseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(GradReverseLayer, self).__init__()\n",
    "\n",
    "    def call(self, x):\n",
    "        return grad_reverse(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder():\n",
    "    inputs  = tf.keras.layers.InputLayer((1000,90,1))\n",
    "    # Layer 1\n",
    "    conv_1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(5,5), strides = (5,5))\n",
    "    norm_1 = tfa.layers.InstanceNormalization(axis=3,center=True,scale=True,beta_initializer=\"random_uniform\",gamma_initializer=\"random_uniform\")\n",
    "    actv_1 = tf.keras.layers.ReLU()\n",
    "    pool_1 = tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,1))\n",
    "    # Layer 2\n",
    "    conv_2 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), strides = (3,3))\n",
    "    norm_2 = tfa.layers.InstanceNormalization(axis=3,center=True,scale=True,beta_initializer=\"random_uniform\",gamma_initializer=\"random_uniform\")\n",
    "    actv_2 = tf.keras.layers.ReLU()\n",
    "    pool_2 = tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,1))\n",
    "    # Layer 3\n",
    "    conv_3 = tf.keras.layers.Conv2D(filters=256, kernel_size=(2,2), strides = (2,2))\n",
    "    norm_3 = tfa.layers.InstanceNormalization(axis=3,center=True,scale=True,beta_initializer=\"random_uniform\",gamma_initializer=\"random_uniform\")\n",
    "    actv_3 = tf.keras.layers.Activation('tanh')\n",
    "    pool_3 = tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,1))\n",
    "    # Latent output\n",
    "    latent = tf.keras.layers.Flatten()\n",
    "    \n",
    "    # model define\n",
    "    mdl = tf.keras.models.Sequential([\n",
    "        inputs,conv_1,norm_1,actv_1,pool_1,\n",
    "        conv_2,norm_2,actv_2,pool_2,\n",
    "        conv_3,norm_3,actv_3,pool_3,latent\n",
    "    ])\n",
    "\n",
    "    return mdl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(latent_shape,output_shape):\n",
    "    \n",
    "    inputs  = tf.keras.layers.InputLayer(latent_shape)\n",
    "    dense_1 = tf.keras.layers.Dense(128)\n",
    "    actv_1  = tf.keras.layers.LeakyReLU()\n",
    "    dense_2 = tf.keras.layers.Dense(output_shape)\n",
    "    actv_2  = tf.keras.layers.Softmax()\n",
    "    \n",
    "    mdl = tf.keras.models.Sequential([inputs,dense_1,actv_1,dense_2,actv_2])\n",
    "    return mdl\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_VGG(num=8,top=True):\n",
    "    base_model = VGG16(include_top=False,input_shape=(1000, 90, 3))\n",
    "    base_model.trainable = False\n",
    "    inputs = tf.keras.layers.Input(shape=(1000, 90, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    p = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    if top == True:\n",
    "        p = tf.keras.layers.Dense(128,activation='relu')(p)\n",
    "        outputs = tf.keras.layers.Dense(num)(p)\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "    else:\n",
    "        model = tf.keras.Model(inputs, p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model \n",
    "\n",
    "encoder = build_encoder()\n",
    "classifier = build_classifier(encoder.output_shape[1:],output_shape=8)\n",
    "model = models.define_graph([encoder,classifier],INPUT_SHAPE)\n",
    "\n",
    "### addition for DANN (gradient reverse)\n",
    "# grad_reverse_layer = GradReverseLayer()\n",
    "# dann_domain_discrimination_graph = model_utilis.define_graph([encoder,grad_reverse_layer,discriminator],INPUT_SHAPE)\n",
    "\n",
    "### VGG \n",
    "# vgg_top = create_VGG(8,False)\n",
    "# _,vgg_clf,vgg_discriminator = build_model(8,normalize=False,seperate=True,latent_shape=vgg_top.output_shape[1:])\n",
    "\n",
    "# vgg_model = model_utilis.define_graph([vgg_top,vgg_clf],(1000,90,3))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sample: 5,(1000,90,1) --> 5,(7,)\n",
    "\n",
    "model(tf.random.uniform(shape=(5,1000,90,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S1.3 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models,utils\n",
    "from dataset import DatasetObject\n",
    "from dataset import import_dataframe as import_experiment_data\n",
    "from external.yousefi.dataset import import_dataframe as import_external_data\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# External dataset (Yousefi,2017)\n",
    "\n",
    "folderpath = \"./external/yousefi/Dataset/Data\"\n",
    "df = import_external_data(folderpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DatasetObject EXT\n",
    "\n",
    "dataset_external = external_dataset.DatasetObject(df,cnn=True,sep='residual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(OneHotEncoder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import Experiment 1\n",
    "folderpath1 = \"./data/exp_1\"  # CHANGE THIS IF THE PATH CHANGED\n",
    "df_exp1 = import_experiment_data(folderpath1) # CHANGE THIS IF THE PATH CHANGED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess for Experiment 1\n",
    "df_exp1_X_ls,df_exp1_y_ls = [],[]\n",
    "for user in df_exp1['user'].unique():\n",
    "    dataframe = df_exp1[df_exp1['user']==user]\n",
    "    features = dataframe[[f'amp_{i}' for i in range(1,91)]].to_numpy()\n",
    "    features = MinMaxScaler().fit_transform(features)\n",
    "    df_exp1_X_ls.append(features)\n",
    "    label = dataframe[['label']].to_numpy()\n",
    "    df_exp1_y_ls.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset object \n",
    "dataset_exp1 = DatasetObject()\n",
    "dataset_exp1.import_data(df_exp1_X_ls,df_exp1_y_ls,window_size=1000,slide_size=200,skip_labels=['noactivity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for CNN\n",
    "dataset_exp1.data_transform(lambda arr: arr.reshape(*arr.shape,1),axis=1,col=0)\n",
    "# Oversampling each fold \n",
    "# dataset_exp1.data_transform(lambda x,y,z : utils.resampling(x,y,z,True),axis=0,col=0)\n",
    "# Encode label \n",
    "dataset_exp1.label_encode(1)\n",
    "# Print shape\n",
    "dataset_exp1.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self dataset EXP1 \n",
    "\n",
    "folderpath1 = \"./data/exp_1\"  # CHANGE THIS IF THE PATH CHANGED\n",
    "df_exp1 = dataset.import_dataframe(folderpath1) # CHANGE THIS IF THE PATH CHANGED\n",
    "# processing step, required\n",
    "df_exp1 = df_exp1.drop([f\"theta_{i}\" for i in range(1,91)], axis=1) \n",
    "print(df_exp1.shape)\n",
    "\n",
    "dataset_exp1 = dataset.DatasetObject(df_exp1,cnn=True,stacking=False)\n",
    "ohe_y = dataset_exp1.one_hot(1)\n",
    "ohe_z = dataset_exp1.one_hot(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self dataset EXP2 \n",
    "\n",
    "\n",
    "folderpath2 = \"./data/exp_2\" # CHANGE THIS IF THE PATH CHANGED\n",
    "df_exp2 = dataset.import_dataframe(folderpath2)\n",
    "# Preprocessing \n",
    "df_exp2['user'] = df_exp2['user'].map(lambda x: x.split('.')[0]) # processing step, required\n",
    "df_exp2 = df_exp2.iloc[:,2:] # processing step, required\n",
    "print(df_exp2.shape)\n",
    "\n",
    "dataset_exp2 = dataset.DatasetObject(df_exp2,cnn=True,stacking=False)\n",
    "ohe_y = dataset_exp2.one_hot(1,ohe_y)\n",
    "ohe_z = dataset_exp2.one_hot(2,ohe_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.concat([df_exp1,df_exp2],axis=0)\n",
    "dataset_exp4 = dataset.DatasetObject(df_,cnn=True,stacking=False)\n",
    "ohe_y = dataset_exp4.one_hot(1)\n",
    "ohe_z = dataset_exp4.one_hot(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_exp1, df_exp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=200)\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### single\n",
    "train,test = dataset_exp1.query([6])\n",
    "history = model.fit(x=train[0],y=train[1],batch_size=64,epochs=100,verbose=1,validation_data=(test[0],test[1]),callbacks=[callback])\n",
    "\n",
    "### cross validation\n",
    "# model,histories,scores = model_utilis.cross_validation(model,dataset_exp1,batch_size=64,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "train,test = dataset_exp1.query([6])\n",
    "cmtx = utils.evaluation(model,test[0],test[1],ohe=dataset_exp1.encoders[0,1])\n",
    "cmtx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "model_name = f\"model_{current_time}\"\n",
    "savepath = f'./saved_model/{model_name}.h5'\n",
    "model.save(savepath)\n",
    "print(\"model saved: \",savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmtx.to_csv(f\"./record/cmtx_{model_name}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
